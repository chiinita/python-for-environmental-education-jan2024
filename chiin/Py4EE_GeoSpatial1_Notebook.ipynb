{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "94c029c2-0951-43c4-b474-95e19a0c0545",
   "metadata": {},
   "source": [
    "# GeoSpatial 1: time series data, interactive maps\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1debba17-0daf-483e-b91b-2a18520d60b2",
   "metadata": {},
   "source": [
    "**<font color='red'>FYI internet access required for Section D and E.</font>**\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3c27396-0cf5-44bd-a98b-68286a6e1a06",
   "metadata": {},
   "source": [
    "Required packages:\n",
    "* pandas\n",
    "* numpy\n",
    "* matplotlib\n",
    "* geopandas\n",
    "* geodatasets\n",
    "* folium\n",
    "* mapclassify"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ba1b56f-9187-4f39-b391-5847b18385f8",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "386bb7ed-6b5f-4cc5-98c6-fdcec098f1c9",
   "metadata": {},
   "source": [
    "**Which sample of Carbon Polluters are we examining?**    \n",
    "US university facilities with large GHG emissions (> 25,000 metric tons of carbon dioxide equivalent (CO 2 e) per year) in any year between 2011 to 2022 (the latest reporting year). "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3266731b-fd74-4db5-8f68-fe285163fdea",
   "metadata": {},
   "source": [
    "**What scientific approaches are we taking?**    \n",
    "Statistical and geospatial approaches."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee6569c0-6701-4075-8c8d-985e80835406",
   "metadata": {},
   "source": [
    "**What outputs will we develop?**    \n",
    "Statistical graphs and interactive maps, with historical and/or regional dimensions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b0192dc-0067-43a5-b8a1-81d83545778b",
   "metadata": {},
   "source": [
    "**What will our outputs tell us?**    \n",
    "Who and where are the significant US sources of carbon pollution within the Higher Education sector, at facility and state level, both recent and since 2012.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fddb4bce-8f1c-489a-82e8-9bf3e2c31825",
   "metadata": {
    "tags": []
   },
   "source": [
    "**Beyond the well-known Eco impacts of Carbon Polluters, what makes this sample significant?**    \n",
    "The fact that this sector has/individual universities are large fossil fuel burners may be a surprise seemingly out of sync with any green credentials or reputation they have garnered, especially relating to clean energy.\n",
    "    \n",
    "For further/corroborating findings, see recent Reuters article -> https://www.reuters.com/investigates/special-report/usa-pollution-universities/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3b99fc0-d779-4f7c-8a68-d36fb1f8534f",
   "metadata": {},
   "source": [
    "---\n",
    "**Data Source - University Emitters**\n",
    "* Filename: `Py4EE_GeoSpatial1_Data.csv`\n",
    "* Org: U.S. Environmental Protection Agency (EPA)\n",
    "* Resource:  Facility Level Information on GreenHouse gases Tool (FLIGHT), which provides information about greenhouse gas (GHG) emissions from large facilities in the U.S., who are required to report annual data about their GHG emissions to EPA as part of the Greenhouse Gas Reporting Program (GHGRP) -> https://ghgdata.epa.gov/ghgp/main.do\n",
    "* Related resources: All GHGRP data products -> https://www.epa.gov/ghgreporting/find-and-use-ghgrp-data\n",
    "\n",
    "**Data Source - US state shapefile**\n",
    "* Tutorial subfolder: `cb_2022_us_state_20m`\n",
    "* Org: U.S. Census Bureau \n",
    "* Resource: Cartographic boundary files -> https://www.census.gov/geographies/mapping-files/time-series/geo/cartographic-boundary.html\n",
    "    * Search for \"States > 1 : 20,000,000 (national) shapefile...\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34f1cd6b-6556-4653-b896-da195a7952a1",
   "metadata": {},
   "source": [
    "## A. Set-up Jupyter Notebook, University Emitters dataset & US state shapefile"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df048f95-f24c-4cfe-8044-2310b3fbc779",
   "metadata": {},
   "source": [
    ">**A0.** Import the required packages and submodule with their conventional aliases.\n",
    ">```\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9687f612-9ada-49dd-ae3e-70ebc3ec0a61",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "937b1b5e-09aa-419c-aaf5-9f9ce7305e94",
   "metadata": {},
   "source": [
    ">**A1.** (OPTIONAL) For autocompletion, or if it's not working, try running this magic command.\n",
    ">```\n",
    "%config Completer.use_jedi = False\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "259507ec-f759-4a7c-b355-7db6b4e9baef",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "311e23ca-39ef-45b6-bff1-b2d1ce4fdd46",
   "metadata": {},
   "source": [
    ">**A2.** Set pandas display options to show all columns/not truncate their display.\n",
    ">```\n",
    "pd.options.display.max_columns = None\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28a778d7-cf47-4ddd-9209-759a002feacd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "06e12de9-0aec-4b71-843b-be6765b410de",
   "metadata": {},
   "source": [
    ">**A3.** Read-in the university emitters dataset `\"Py4EE_GeoSpatial1_Data.csv\"`, and assign to `raw_data`.    \n",
    ">\n",
    ">**Code Detail:** Although we are still passing in just the path to the file as the only required parameter of the `read_csv()` method, note from the Docstring the range of options available for customising the call.\n",
    ">```\n",
    "raw_data = pd.read_csv(\"Py4EE_GeoSpatial1_Data.csv\")\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a45f8ffc-918d-4dd4-9bd0-429fc2af6e8d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "2beba175-7144-4e3b-946e-f12b550329fb",
   "metadata": {},
   "source": [
    ">**A4.** Copy `raw_data` to create the `DataFrame` we will be prepping called `df_prep`.\n",
    ">```\n",
    "df_prep = raw_data.copy()\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21f6813b-ac4f-4b38-928d-61604ba404d8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ec1d740c-337b-4eea-ab2d-fad90fb7f852",
   "metadata": {},
   "source": [
    ">**A5.** Ensure the US state shapefile `cb_2022_us_state_20m` is a subfolder in your current working directory (or locally accessible).  \n",
    "> * This shapefile is an Esri vector data storage format that stores the official location, shape, and attributes for each state at the 1 : 20,000,000 (national) ratio scale. \n",
    "> * `cb_2022_us_state_20m` contains a set of related files, such as the `.shp`, `.shx`, `.dbf`, and `.prj` files components of the shapefile.\n",
    "> * Download here -> https://www2.census.gov/geo/tiger/GENZ2022/shp/cb_2022_us_state_20m.zip"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35338cf6-8c4a-4a29-a37f-04c2dec2ebdf",
   "metadata": {},
   "source": [
    "---\n",
    "## B. Inspect the University Emitters dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "568182bb-7e8e-4df7-b3c7-af94418bd841",
   "metadata": {},
   "source": [
    ">**B0.** Have a look at the dataset of large carbon polluting US university facilities.\n",
    ">\n",
    ">**Code Detail:** Instead of `head()` returning a default number of rows (5), start specifying the `n` argument.\n",
    ">```\n",
    "df_prep.head(2)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3067d89f-88ac-47d3-ae9d-79983d6e240f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e6f785da-dbc6-4b4a-8472-709cd88f1184",
   "metadata": {},
   "source": [
    "<font color=\"green\">***B0. Comments***      \n",
    "*- There is missing data that requires data prep (also called cleaning/munging/wrangling).*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72ce821f-5d5c-45d4-8020-ba04efee00f9",
   "metadata": {},
   "source": [
    ">**B1.** Find out the dimensions of `df_prep`.\n",
    ">```\n",
    "df_prep.shape\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "345c0fcc-cf8c-4e49-ab6a-377f13e63e79",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "31e57737-cb85-40ba-843a-7ed8c88df7d5",
   "metadata": {},
   "source": [
    "<font color=\"green\">***B1. Comment***     \n",
    "*- There are 123 US university facilities which were large carbon polluters in at least one of the years between 2011-2022.*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc9696ed-68d1-40f9-8ae8-77bb212aa6d2",
   "metadata": {},
   "source": [
    ">**B2.** (OPTIONAL) Find out which US university we saw in `GeoSpatial0` had the worst 5-year total pollution, i.e. retrieve the row where the `\"GHGRP ID\"` is 1001250.\n",
    ">```\n",
    "df_prep[df_prep[\"GHGRP ID\"] == 1001250]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15d59d46-c3f3-4fd1-9d6b-002adef1dca0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "189cb53f-aa5a-4dfb-ad27-e8ebefc99e56",
   "metadata": {},
   "source": [
    "---\n",
    "## C. Prepare the University Emitters dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "980610d2-b137-4eee-91ab-42f91a319c6f",
   "metadata": {},
   "source": [
    ">**C0.** Some of the columns are definitely not needed, so let's prepare to drop them, and with some efficiency. Instead of manually counting, let's programmatically find out the index positions of each `df_prep` column.\n",
    ">```\n",
    "list(enumerate(df_prep.columns))\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11b113f3-200a-4391-9ec1-333dee5d339e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "5c8ee854-3176-436b-b038-ebc2a5ff9a01",
   "metadata": {},
   "source": [
    ">**C1.** Perform a drop operation that removes 4 particular columns from `df_prep` inplace (`SUBPARTS`, `CHANGE IN EMISSIONS (2021 TO 2022)`, `CHANGE IN EMISSIONS (2011 TO 2022)`, and `SECTORS`). Review the modification.    \n",
    ">\n",
    ">**Code Detail:** Pass the relevant subset of the `df_prep.columns` attribute as the named `columns` argument of the `drop()` method.    \n",
    ">\n",
    ">**Tech Note:** The `inplace` parameter is accepted by several pandas methods is used in this Tutorial where possible for brevity, but it's existence is controversial and currently in flux.\n",
    ">```\n",
    "df_prep.drop(columns = df_prep.columns[[10,23,24,25]], inplace=True)\n",
    "df_prep.head(2)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f50600d-1ef0-431f-a8f2-0ff9c8074646",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "3e96d1fc-55e4-44af-8591-baecf89bc38d",
   "metadata": {},
   "source": [
    ">**C2.** See what `dtype` pandas inferred was in each column when it originally read-in the `Py4EE_GeoSpatial1_DATA.csv`.\n",
    ">```\n",
    "df_prep.dtypes\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5900f05-b744-49ad-b66f-b2aabc0f9f8a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "674839d7-0a42-40ac-8352-5c397d4b995a",
   "metadata": {},
   "source": [
    "<font color=\"green\">***C2. Comment***     \n",
    "*- pandas' inferences are only partial accurate. The columns that are essential to correct are the 12 years of reported emissions data. These are all currently `object` `dtype`, so basically string data, not numeric as required.*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9b320f1-1cd2-42ab-b522-1f9524f4135d",
   "metadata": {},
   "source": [
    ">**C3.** Before we start modifying the 12 reported emissions data columns, let's shorten their long, cumbersome labels to just the reporting year reference. Review the modification.    \n",
    ">\n",
    ">**Code Detail:** Perform a renaming operation inplace where a `replace()` string method that returns a copy of the original string with the substring `\"TOTAL REPORTED EMISSIONS, \"` replaced by nothing is applied to each column label of `df_prep`.\n",
    ">```\n",
    "df_prep.rename(columns = lambda x: x.replace(\"TOTAL REPORTED EMISSIONS, \", \"\"), inplace=True)\n",
    "df_prep.head(2)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a4b94be-e7d2-4180-bbea-b2519e4198f2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "7d99bb17-5e42-4c51-93bc-ea8d44a72d4c",
   "metadata": {},
   "source": [
    ">**C4.** Now start the process towards converting these 12 columns to a numeric `dtype`. Remove the comma characters from the string data in these 12 columns, but not anywhere else in `df_prep` (e.g. `\"REPORTED ADDRESS\"`). Review the modification.\n",
    "> \n",
    ">**Code Detail:** Use pandas `loc` indexer to select these 12 `df_prep` columns by inputting a slice object with labels after the comma. `map()` applies a function to a Dataframe elementwise, as opposed to row/column-wise.    \n",
    ">```\n",
    "df_prep.loc[:, \"2011\":] = df_prep.loc[:, \"2011\":].map(lambda x: x.replace(\",\", \"\"))\n",
    "df_prep.head(2)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4b2baa6-3180-4637-8ddb-3f7e2e9768e0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "9626cb1a-09dc-4670-9fcb-e90ae870a54a",
   "metadata": {},
   "source": [
    "<font color=\"green\">***C4. Comment***        \n",
    "*- As a specific example of the general point that there are typically **multiple ways to do the same thing in scientific Python**, other ways we can select these 12 priority columns include:*\n",
    "```\n",
    "df_prep.iloc[:, -12:]\n",
    "df_prep.filter(regex=\"^20\")\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57e267a0-6da0-4c09-8f54-cba7d5d03ab8",
   "metadata": {},
   "source": [
    ">**C5.** The last string clean-up step is to deal with the `\"---\"` instances in the 12 columns that we assume is EPA notation for `N/A`. We will replace these string values inplace with `NaN`. Review the modification.\n",
    ">```\n",
    "df_prep.replace(\"---\", np.nan, inplace=True)\n",
    "df_prep.head(2)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b7d79fc-0ce4-4c9b-89cd-b3f790ec0880",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "7adcb16f-cd72-4178-bfdf-eaedcefc8091",
   "metadata": {},
   "source": [
    ">**C6.** Now convert the 12 columns to a numeric `dtype`. Review the modification by eye.  \n",
    ">\n",
    ">```\n",
    "df_prep[df_prep.columns[-12:]] = df_prep[df_prep.columns[-12:]].apply(pd.to_numeric)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac3db463-aeca-4cd4-90cf-1dea92ed1863",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "4177dc36-884a-4f6d-96d7-620d64993e0b",
   "metadata": {},
   "source": [
    ">**C7.** Now review the modification more formally by accessing the `dtypes` attribute again.\n",
    ">```\n",
    "df_prep.dtypes\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7aafcef2-85b0-4103-a562-f39dee4c29be",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "5d657b76-4d4e-4ea6-88c7-8f9527623035",
   "metadata": {},
   "source": [
    ">**C8.** Now that the columns with the 12 years of reported emissions data are a numeric `dtype` (as well as some other columns), let's compute some quick summary statistics.    \n",
    ">\n",
    ">**Tech Note:** To show less/no decimal places use `pd.set_option (\"display.precision\", 0)`\n",
    ">```\n",
    "df_prep.describe()\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ae5c170-4306-400b-aaec-22f998d36b8f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "98b46667-ec26-45b0-9c40-ec27c6bd6632",
   "metadata": {},
   "source": [
    ">**C9.** The subset of 12 emissions columns is time series data. Generate a quick matplotlib line plot.\n",
    ">\n",
    ">**Code Detail:** Access the `T` attribute of the `df_prep` subset to return the transpose, then call the `plot()` method, using the `legend` keyword to not place a legend on the plot.  \n",
    ">\n",
    ">**Tech Note:** The `plot()` method for `DataFrame` or `Series` data structures uses the backend specified by the option `plotting.backend`, which has the default value of matplotlib.\n",
    ">```\n",
    "df_prep[ df_prep.columns[-12:]].T.plot(legend=False)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac203436-5ac0-45f4-80f8-e8a2fe3ccaa0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f2b9c9f3-aa6a-4439-9d24-01d3ef86092b",
   "metadata": {},
   "source": [
    "<font color=\"green\">***C9. Comment***    \n",
    "*- By eye it looks like all data points in the time series are above the 25,000 metric ton CO 2 e threshold as specified in the original EPA FLIGHT data request, but a programmatic check can optionally be performed in **C10.***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef600816-c556-424a-856c-cb5674bad731",
   "metadata": {},
   "source": [
    ">**C10.** (OPTIONAL) Check that all the data points in the time series are either greater than 25,000 or `NaN` by generating a Boolean array for the conditions then determining whether all the values are `True` or not.\n",
    ">```\n",
    "( (df_prep.iloc[:, -12:] > 25000) | (df_prep.iloc[:, -12:].isna()) ).values.all()\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98b63b03-900d-4c7a-bf11-3f92a72926f4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f9f267dc-81d3-4e6a-8e48-1f021431133c",
   "metadata": {
    "tags": []
   },
   "source": [
    ">**C11.** Final task for **Section C.**, create a new `\"Cumulative\"` column with the sum of each facility's reported emissions data over the 12 years. Review the modification.\n",
    ">\n",
    ">**Code Detail:** Extend `df_prep` by assigning a new index value, `\"Cumulative\"` using the indexing operator. To perform the `sum()` operation on the columns axis a named `axis` argument of either `columns` or `1` must be passed.\n",
    ">```\n",
    "df_prep[\"Cumulative\"] = df_prep.loc[:, \"2011\":].sum(axis=1)\n",
    "df_prep.head(2)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa22fb1e-a4c9-465e-9764-a3814c00779d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c2134804-a9fb-41f3-9df5-f3232763c23f",
   "metadata": {},
   "source": [
    ">**C12.** (OPTIONAL) Calculate the total GHG emissions/volume of carbon pollution that the large facilities in the US university sector have been responsible for creating over the reported 12-year period.\n",
    ">```\n",
    "df_prep[\"Cumulative\"].sum()\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5ccc432-77b7-4cd1-97ca-b9db23ea37fb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec8c6d26-e285-4ce5-8602-e463169cd339",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "73e0608c-6421-45b6-9d2b-bcacaee7e076",
   "metadata": {},
   "source": [
    "<font color=\"green\">***C12. Comment***     \n",
    "*- These 123 US university facilities are responsible for ~105 million tons of carbon pollution from 2011 to 2022.*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73e6f703-812a-4fff-8ac7-e2c84f269f54",
   "metadata": {},
   "source": [
    "---\n",
    "## D. Map dataset for regional trends - Basic"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45e49c30-65f2-440d-87b1-6683add1477a",
   "metadata": {},
   "source": [
    "<font color=\"green\">***D. Intro***       \n",
    "*- For this mapping section D. and E. we use the geographic pandas extension, geopandas, to create a `GeoDataFrame` object.*       \n",
    "*- A `GeoDataFrame` is a pandas `DataFrame` that has a column with geometry, and extends pandas functionality in order to make basic maps.*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05ec5d64-0452-4349-a866-24da234f9034",
   "metadata": {},
   "source": [
    ">**D0.** Import geopandas with it's conventional alias and geodatasets.\n",
    ">```\n",
    "import geopandas as gpd\n",
    "import geodatasets\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "caa59508-8bf9-46c8-be5c-93601c5840f9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d86f4012-10f6-42e6-a850-b534e45d76fa",
   "metadata": {},
   "source": [
    ">**D1.** Create a `GeoDataFrame` called `geo_df` using `df_prep`. Review the new object.\n",
    "> \n",
    ">**Code Detail:** Call geopandas `GeoDataFrame()` function, inputting `df_prep` as well as a `geometry` keyword argument which is another geopandas function `points_from_xy()` called with `df_prep`'s `\"LONGITUDE\"` and `\"LATITUDE\"` columns as the required positional `x` and `y` arguments respectively.    \n",
    ">```\n",
    "geo_df = gpd.GeoDataFrame(df_prep, geometry=gpd.points_from_xy(df_prep[\"LONGITUDE\"], df_prep[\"LATITUDE\"]))\n",
    "geo_df.head()\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93a5728c-f335-4ad6-b86d-54e63e48c88f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "0a6cf05b-5ddb-494f-8889-3213a5ca8908",
   "metadata": {},
   "source": [
    ">**D2.** Generate a default plot of this new `GeoDataFrame` `geo_df`. Then try a customised plot where the colour of the points is based on their `\"Cumulative\"` column value. Finally try customising the colormap, `cmap`, used that reflect the `\"Cumulative\"` values.\n",
    "> \n",
    ">**Tech Note:** geopandas uses matplotlib for this `plot()` method. matplotlib has a range of built-in colormaps -> https://matplotlib.org/stable/tutorials/colors/colormaps.html\n",
    ">```\n",
    "geo_df.plot()\n",
    "geo_df.plot(column=\"Cumulative\")\n",
    "geo_df.plot(column=\"Cumulative\", cmap=\"cool\")\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a443b883-9184-4d42-85fe-9af8025483fd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b7995608-ed7d-4ba7-bd47-1264b49331ce",
   "metadata": {},
   "source": [
    ">**D3.** Evidently, `geo_df` plot needs a base map to contextualise the points. Download one of `geodatasets` available datasets, `\"naturalearth.land\"`, assign to `base_map`, then plot. \n",
    ">\n",
    ">**Tech Notes:** `base_map` is also a `GeoDataFrame` object. Run `geodatasets.data` for full list of datasets.\n",
    ">```\n",
    "base_map = gpd.read_file(geodatasets.get_path(\"naturalearth.land\"))\n",
    "base_map.plot()\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f7b7474-47e6-4064-9e66-aa3171281032",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ce8eb6b9-7fc8-4a10-b8dc-bf86ddc4a29c",
   "metadata": {},
   "source": [
    ">**D6.** Use matplotlib to construct a basic map that plots `geo_df` and `base_map` together, visualising the location of the 123 large university emitters and the relative size of their cumulative emissions over 2011-2022. \n",
    ">\n",
    ">**Tech Note:** Many matplotlib plotting routines start with `fig, ax = plt.subplots()`, even when the output is a single plot, as a `Figure` and an `Axes` object is created in one step. `Axes` is not an `axis` reference, rather an instance of the class `plt.Axes` that is a bounding box object with ticks and labels. Conventionally, `ax` is used to refer to an individual axes instance, or a group of axes instances...!!\n",
    ">```\n",
    "fig, ax = plt.subplots(figsize=(12,8))    # Create an empty matplotlib Figure and Axes  \n",
    "base_map.plot(ax=ax)\n",
    "geo_df.plot(ax=ax, column=\"Cumulative\", cmap=\"autumn_r\")    # Other input options include legend=True, legend_kwds={\"orientation\": \"horizontal\"}\n",
    "#plt.tight_layout()\n",
    "#plt.savefig(\"<SomeFilename>.png\", dpi=600)    # Save the Figure/Axes using matplotlib - use the optional dpi (dots-per-inch) argument to control the resolution of the png\n",
    "plt.show()    # Display plot\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bcedc51-800a-4e36-a74a-4df6eeecbe22",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "10607502-d670-4e9e-a0f3-adcfc3ab4190",
   "metadata": {},
   "source": [
    "---\n",
    "## E. Map dataset for regional trends - Interactive"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cddc1c2-30c9-4174-a6e8-826050623640",
   "metadata": {},
   "source": [
    "<font color=\"green\">***E. Intro***    \n",
    "*- In this section we extend our geopandas extension of pandas with folium, an optional geopandas dependency which provides interactive mapping from the Open-Source leaflet.js library via a Python interface.*         \n",
    "*- See geopandas docs -> https://geopandas.org/en/stable/gallery/plotting_with_folium.html*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5e5f5fa-23f3-464d-b76c-37521a75b2fa",
   "metadata": {},
   "source": [
    ">**E0.** Import folium.\n",
    ">```\n",
    "import folium\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe93ecc0-fff0-42f8-ab7e-8898481175c7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "450a482a-0c80-46c5-8223-7bda52c8b728",
   "metadata": {},
   "source": [
    ">**E1.** Test out the extended geopandas functionality that folium now enables. Call `explore()` on `base_map`.\n",
    ">\n",
    ">**Tech Node:** The leaflet/folium maps takes a moment to render.\n",
    ">```\n",
    "base_map.explore()\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20255fae-134f-49af-80d9-c610e793f101",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c7878f0a-0bca-4962-9c62-e0f4e8c7fd33",
   "metadata": {},
   "source": [
    ">**E2.** Rather than using a world map as a base layer to plot US-only data, let's prepare a regional map. Read-in the US Census states shapefile we double-checked in **Section A.**, creating another `GeoDataFrame`, and assign to `US_state_boundaries`.    \n",
    ">\n",
    ">**Tech Note:** Leave the `\"cb_2022_us_state_20m\"` directory and files as-is, they contain dependencies.\n",
    ">```\n",
    "US_state_boundaries = gpd.read_file(\"cb_2022_us_state_20m/cb_2022_us_state_20m.shp\")\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "704f3c2c-b5cc-47b4-884b-bab295443f8d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "5cc6c788-9cd2-48f5-b938-e48a5c18fdb6",
   "metadata": {},
   "source": [
    ">**E3.** (OPTIONAL) `US_state_boundaries` is another `GeoDataFrame` like `base_map` with multi/polygons, although for US states rather than world countries. What folium now allows us to do is visualize `GeoDataFrame` data on a leaflet map. Try calling `explore()` again.    \n",
    ">```\n",
    "US_state_boundaries.explore()\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45f8f7f8-bc28-4a71-8600-df411ab4f7d4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "09f0d929-c79d-43ec-8410-823246dc470a",
   "metadata": {},
   "source": [
    ">**E4.** Through a plotting routine harnessing folium-extended capabilities of geopandas with `geo_df` and `US_state_boundaries`, construct an improved version of our **D6.** basic map, namely an interactive map that visualises the location of the 123 large university emitters and the relative size of their cumulative emissions over 2011-2022.\n",
    ">\n",
    ">**Code Details:** \n",
    ">* Create a `base_layer` which is an `US_state_boundaries.explore()` object, but setting several optional arguments that adjust formatting and interactivity.    \n",
    ">* Then call `explore()` on `geo_df`, specifying this plot to be drawn on the existing map instance, `base_layer`, using the `m` keyword and `\"Cumulative\"` as the column to plot. Use available keyword arguments to fine-tune formatting and interactivity.\n",
    ">* Finally, display `base_layer`.\n",
    ">\n",
    ">**Tech Notes:**\n",
    "> * The `radius` of 4,828 meters set in `marker_kwds` is significant because this is the value used in EPA material on GHGRP facilities and their impact on surrounding communities, namely 3 miles -> https://edap.epa.gov/public/extensions/GHGRP-Demographic-Data-Highlights/GHGRP-Demographic-Data-Highlights.html\n",
    "> * min_zoom refers to how far out (smaller int), max zoom refers to how far in (larger int)\n",
    ">```\n",
    "base_layer = US_state_boundaries.explore(location=[39, -97], width=\"60%\", height=\"60%\", zoom_start=4, min_zoom=3, tooltip=False, style_kwds=dict(fillOpacity=1, weight=1, fillColor=\"gainsboro\"), highlight_kwds=dict(fillOpacity=0, weight=3))\n",
    ">\n",
    ">geo_df.explore(m=base_layer, column=\"Cumulative\", marker_type=\"circle\", marker_kwds=dict(radius=4828, fill=False), cmap=\"autumn_r\", popup=[\"FACILITY\", \"Cumulative\"], tooltip=False)\n",
    ">\n",
    ">#base_layer.save(\"<SomeFilename>.html\")    # Save map, e.g. as html\n",
    "base_layer\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "856d4b8b-31e4-4f7f-bee3-1b89a0fd1954",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  },
  "nteract": {
   "version": "0.28.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
